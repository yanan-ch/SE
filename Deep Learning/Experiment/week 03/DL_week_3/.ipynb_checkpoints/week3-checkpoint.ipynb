{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOGRAD: AUTOMATIC DIFFERENTIATION:\n",
    "\n",
    "    Central to all neural networks in PyTorch is the autograd package. Let’s first briefly visit this, and we\n",
    "    will then go to training our first neural network.\n",
    "    \n",
    "    The autograd package provides automatic differentiation for all operations on Tensors. It is a define-by-\n",
    "    run framework, which means that your backprop is defined by how your code is run, and that every single \n",
    "    iteration can be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tensor and set requires_grad=True to track computation with it\n",
    "    torch.Tensor is the central class of the package. If you set its attribute .requires_grad as True, it \n",
    "    starts to track all operations on it. When you finish your computation you can call .backward() and have \n",
    "    all the gradients computed automatically. The gradient for this tensor will be accumulated into .grad \n",
    "    attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "x1 = torch.ones(2,2,requires_grad=False)\n",
    "# x1.requires_grad_(True)\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a tensor operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward>)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "y = x+2\n",
    "print(y)\n",
    "y1 = x1 +2\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## y was created as a result of an operation, so it has a grad_fn.But y1 not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward object at 0x7f7324638940>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)\n",
    "print(y1.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do more operations on y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward>) tensor(27., grad_fn=<MeanBackward1>)\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]]) tensor(27.)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "z1 = y1*y1*3\n",
    "out = z.mean()   #calculate z average value\n",
    "out1 = z1.mean()   #calculate z1 average value\n",
    "\n",
    "print(z, out)\n",
    "print(z1,out1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .requires_grad_( ... ) changes an existing Tensor’s requires_grad flag in-place. The input flag defaults to False if not given.\n",
    "    Tensor and Function are interconnected and build up an acyclic graph, that encodes a complete history of \n",
    "    computation. Each tensor has a .grad_fn attribute that references a Function that has created the Tensor \n",
    "    (except for Tensors created by the user - their grad_fn is None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x7f732461fcf8>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients:\n",
    "\n",
    "    Let’s backprop now. Because out contains a single scalar, out.backward() is equivalent to \n",
    "    out.backward(torch.tensor(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()\n",
    "# out.backward(torch.tensor(1.))\n",
    "out1.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print gradients d(out)/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let’s take a look at an example of Jacobian-vector product:\n",
    "    If you want to compute the derivatives, you can call .backward() on a Tensor. If Tensor is a scalar (i.e. \n",
    "    it holds a one element data), you don’t need to specify any arguments to backward(), however if it has \n",
    "    more elements, you need to specify a gradient argument that is a tensor of matching shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(3,requires_grad=True)\n",
    "y = x + torch.tensor([1.,2.,3.])\n",
    "z = y*y*y\n",
    "print(z)\n",
    "v = torch.tensor([1,0.1,0.01])\n",
    "z.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## you can also stop autograd from tracking history on Tensors with .requires_grad=True by wrapping the code block in with torch.no_grad():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.requires_grad)\n",
    "print((x**2).requires_grad)\n",
    "with torch.no_grad():\n",
    "    print(x.requires_grad)\n",
    "    print((x**2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORKS\n",
    "\n",
    "    A typical training procedure for a neural network is as follows:\n",
    "    \n",
    "        ·Define the neural network that has some learnable parameters (or weights)\n",
    "        ·Iterate over a dataset of inputs\n",
    "        ·Process input through the network\n",
    "        ·Compute the loss (how far is the output from being correct)\n",
    "        ·Propagate gradients back into the network’s parameters\n",
    "        ·Update the weights of the network, typically using a simple update rule: weight = weight - \n",
    "        learning_rate * gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network\n",
    "    Let’s define this network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = torch.unsqueeze(torch.linspace(-3, 3, 100), dim=1)  # x data (tensor), shape=(100, 1)\n",
    "# y = x.pow(2) + 0.2 * torch.rand(x.size())  # noisy y data (tensor), shape=(100, 1)\n",
    "y = torch.sin(x) + 0.5 * torch.rand(x.size())\n",
    "\n",
    "\n",
    "\n",
    "x, y = Variable(x), Variable(y)\n",
    "\n",
    "\n",
    "# 打印生成的散点图\n",
    "# plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "# plt.show()\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()  # 继承init功能\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)  # 隐藏层线性输出\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)  # 输出层线性输出\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 正向传播输入值，神经网络分析输出值\n",
    "        x = F.relu(self.hidden(x))  # 激励函数隐藏层的线性值）\n",
    "        x = self.predict(x)  # 输出值\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net(n_feature=1, n_hidden=10, n_output=1)\n",
    "print(net)\n",
    "\n",
    "plt.ion()\n",
    "plt.show()\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)  # 传入net所学习的参数，学习速度\n",
    "loss_func = torch.nn.MSELoss()  # 均方差公式\n",
    "\n",
    "for t in range(500):\n",
    "    prediction = net(x)  # 喂给net训练数据x,输出预测值\n",
    "    print(prediction)\n",
    "    loss = loss_func(prediction, y)  # 计算两者误差\n",
    "    optimizer.zero_grad()  # 清空上一步的残余更新参数\n",
    "    loss.backward()  # 误差反向传播，计算参数更新值\n",
    "    optimizer.step()  # 将参数更新值施加到net的parameter上\n",
    "    if t % 10 == 0:\n",
    "        plt.cla()\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())             #打印散点图\n",
    "        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)      #打印拟合的曲线\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color': 'red'})\n",
    "        plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA LOADING AND PROCESSING TUTORIAL\n",
    "\n",
    "    A lot of effort in solving any machine learning problem goes in to preparing the data. PyTorch provides \n",
    "    many tools to make data loading easy and hopefully, to make your code more readable. In this tutorial, we \n",
    "    will see how to load and preprocess/augment data from a non trivial dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To run this tutorial, please make sure the following packages are installed:\n",
    "    \n",
    "    ·scikit-image: For image io and transforms\n",
    "     $  sudo apt-get install python-numpy  \n",
    "     $  sudo apt-get install python-scipy  \n",
    "     $  sudo apt-get install python-matplotlib\n",
    "     \n",
    "     $  sudo pip install  scikit-image  \n",
    "    \n",
    "    ·pandas: For easier csv parsing\n",
    "    $  sudo apt-get install python-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let’s quickly read the CSV and get the annotations in an (N, 2) array where N is the number of landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_frame = pd.read_csv('data/faces/face_landmarks.csv')\n",
    "\n",
    "n = 0\n",
    "img_name = landmarks_frame.iloc[n, 0]\n",
    "landmarks = landmarks_frame.iloc[n, 1:].as_matrix()\n",
    "landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "\n",
    "print('Image name: {}'.format(img_name))\n",
    "print('Landmarks shape: {}'.format(landmarks.shape))\n",
    "print('First 4 Landmarks: {}'.format(landmarks[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_landmarks(image, landmarks):\n",
    "    \"\"\"Show image with landmarks\"\"\"\n",
    "    plt.imshow(image)\n",
    "    plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "plt.figure()\n",
    "show_landmarks(io.imread(os.path.join('data/faces/', img_name)),\n",
    "               landmarks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceLandmarksDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.landmarks_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:].as_matrix()\n",
    "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let’s instantiate this class and iterate through the data samples. We will print the sizes of first 4 samples and show their landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_dataset = FaceLandmarksDataset(csv_file='data/faces/face_landmarks.csv',\n",
    "                                    root_dir='data/faces/')\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(len(face_dataset)):\n",
    "    sample = face_dataset[i]\n",
    "\n",
    "    print(i, sample['image'].shape, sample['landmarks'].shape)\n",
    "\n",
    "    ax = plt.subplot(1, 4, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Sample #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "    show_landmarks(**sample)\n",
    "\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

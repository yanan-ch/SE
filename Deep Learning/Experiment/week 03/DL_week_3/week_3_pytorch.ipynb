{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. AUTOGRAD: AUTOMATIC DIFFERENTIATION:\n",
    "\n",
    "Central to all neural networks in PyTorch is the **autograd** package. Let’s first briefly visit this, and we\n",
    "will then go to training our first neural network.\n",
    "    \n",
    "The **autograd** package provides automatic differentiation for all operations on Tensors. It is a define-by-\n",
    "run framework, which means that your backprop is defined by how your code is run, and that every single \n",
    "iteration can be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Create a tensor and set requires_grad=True to track computation with it:\n",
    "torch.Tensor is the central class of the package. If you set its attribute **.requires_grad** as **True**, it starts to track all operations on it. When you finish your computation you can call **.backward()** and have all the gradients computed automatically. The gradient for this tensor will be accumulated into **.grad** attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a function:  \n",
    "x = \\[[1,1],[1,1]]  \n",
    "y = x + 2  \n",
    "z = y ^ 2 * 3  \n",
    "out = z.mean()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# create a tensor with setting its .requires_grad as Ture\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "x1 = torch.ones(2,2,requires_grad=False)\n",
    "# x1.requires_grad_(True)\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Do a tensor operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "y1 = x1 + 2\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y was created as a result of an operation, so it has a grad_fn.But y1 not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.grad_fn)\n",
    "print(y1.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Do more operations on y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y * y * 3\n",
    "z1 = y1 * y1 * 3\n",
    "out = z.mean()   #calculate z average value\n",
    "out1 = z1.mean()   #calculate z1 average value\n",
    "\n",
    "print(z, out)\n",
    "print(z1, out1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**.requires_grad_( )** changes an existing Tensor’s requires_grad flag in-place. The input flag defaults to False if not given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor and Function are interconnected and build up an acyclic graph, that encodes a complete history of \n",
    "computation. Each tensor has a .grad_fn attribute that references a Function that has created the Tensor \n",
    "(**except for Tensors created by the user - their grad_fn is None**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 2)    # a is created by user, its .grad_fn is None\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)   # change the attribute .grad_fn of a\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()        # add all elements of a  to b\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gradients:\n",
    "\n",
    "## 2.1. Let’s backprop now. \n",
    "Because out contains a single scalar, **out.backward( )** is equivalent to **out.backward(torch.tensor(1.))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()\n",
    "# out.backward(torch.tensor(1.))\n",
    "# out1.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can get parameters gradient like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grad = x.grad\n",
    "y_grad = y.grad\n",
    "z_grad = z.grad\n",
    "print(x_grad)\n",
    "print(y_grad)\n",
    "print(z_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Now let’s take a look at an example of Jacobian-vector product:\n",
    "If you want to compute the derivatives, you can call **.backward( )** on a Tensor. If Tensor is a **scalar** (i.e. it holds a one element data), you don’t need to specify any arguments to backward( ), however if it has more elements, you need to specify a gradient argument that is a tensor of matching shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a function:  \n",
    "x = \\[1, 1, 1]  \n",
    "y = x + \\[1, 2, 3]  \n",
    "z = y ^ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(3, requires_grad=True)\n",
    "y = x + torch.tensor([1., 2., 3.])\n",
    "z = y * y * y\n",
    "print(z)\n",
    "\n",
    "v = torch.tensor([1, 0.1, 0.01])\n",
    "# z is a vector, so you need to specify a gradient whose size is the same as z\n",
    "z.backward(v)    \n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题1：\n",
    "**传入 .backward()里面的tensor是什么？请尝试不同的输入并回答。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. NEURAL NETWORKS\n",
    "\n",
    "A typical training procedure for a neural network is as follows:\n",
    "    \n",
    "- Define the neural network that has some learnable parameters (or weights)\n",
    "- Iterate over a dataset of inputs\n",
    "- Process input through the network\n",
    "- Empty the parameters in optimizer\n",
    "- Compute the loss (how far is the output from being correct)\n",
    "- Propagate gradients back into the network’s parameters\n",
    "- Update the weights of the network, typically using a simple update rule: weight = weight - learning_rate * gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Define the network\n",
    "Let’s define a network to classify points of gaussian distribution to three class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Show all points\n",
    "Show all points(containing trainset and testset) you will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all points, you can skip this cell\n",
    "def show_original_points():\n",
    "    label_csv = open('./labels/label.csv', 'r')\n",
    "    label_writer = csv.reader(label_csv)\n",
    "    class1_point = []\n",
    "    class2_point = []\n",
    "    class3_point = []\n",
    "    for item in label_writer:\n",
    "        if item[2] == '0':\n",
    "            class1_point.append([item[0], item[1]])\n",
    "        elif item[2] == '1':\n",
    "            class2_point.append([item[0], item[1]])\n",
    "        else:\n",
    "            class3_point.append([item[0], item[1]])\n",
    "    data1 = np.array(class1_point, dtype=float)\n",
    "    data2 = np.array(class2_point, dtype=float)\n",
    "    data3 = np.array(class3_point, dtype=float)\n",
    "    x1, y1 = data1.T\n",
    "    x2, y2 = data2.T\n",
    "    x3, y3 = data3.T\n",
    "    plt.figure()\n",
    "    plt.scatter(x1, y1, c='b', marker='.')\n",
    "    plt.scatter(x2, y2, c='r', marker='.')\n",
    "    plt.scatter(x3, y3, c='g', marker='.')\n",
    "    plt.axis()\n",
    "    plt.title('scatter')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Define a network\n",
    "When you define a network, your class must to inherit **nn.Moudle**, then you should to overload **\\_\\_init__** method and **forward** method\n",
    "\n",
    "Network(  \n",
    "    (hidden): Linear(in_features=2, out_features=5, bias=True)  \n",
    "    (sigmiod): Sigmoid()  \n",
    "    (predict): Linear(in_features=5, out_features=3, bias=True)  \n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        '''\n",
    "        Args:\n",
    "            n_feature(int): size of input tensor\n",
    "            n_hidden(int): size of hidden layer \n",
    "            n_output(int): size of output tensor\n",
    "        '''\n",
    "        super(Network, self).__init__()\n",
    "        # define a liner layer\n",
    "        self.hidden = nn.Linear(n_feature, n_hidden)\n",
    "        # define sigmoid activation \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.predict = nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x(tensor): inputs of the network\n",
    "        '''\n",
    "        # hidden layer\n",
    "        h1 = self.hidden(x)\n",
    "        # activate function\n",
    "        h2 = self.sigmoid(h1)\n",
    "        # output layer\n",
    "        out = self.predict(h2)\n",
    "        '''\n",
    "        Linear classifier often follows softmax to output probability,\n",
    "        however the loss function CrossEntropy we used have done this \n",
    "        operation, so we don't use softmax function here.\n",
    "        '''\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrossEntropy written in pytorch:\n",
    "[https://pytorch.org/docs/stable/nn.html?highlight=crossentropy#torch.nn.CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html?highlight=crossentropy#torch.nn.CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Overload a Dataset\n",
    "Please skip the below cell when you are trying to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        '''\n",
    "        Args:\n",
    "            csv_file(string): path of label file\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        '''\n",
    "        self.frame = pd.read_csv(csv_file, encoding='utf-8', header=None)\n",
    "        print('csv_file source ---->', csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.frame.iloc[idx, 0]\n",
    "        y = self.frame.iloc[idx, 1]\n",
    "        point = np.array([x, y])\n",
    "        label = int(self.frame.iloc[idx, 2])\n",
    "        if self.transform is not None:\n",
    "            point = self.transform(point)\n",
    "        sample = {'point': point, 'label': label}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4. Train function\n",
    "Train a model and show running_loss curve ana show accuracy curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(classifier_net, trainloader, testloader, device, lr, optimizer):\n",
    "    '''\n",
    "    Args:\n",
    "        classifier_net(nn.model): train model\n",
    "        trainloader(torch.utils.data.DateLoader): train loader\n",
    "        testloader(torch.utils.data.DateLoader): test loader\n",
    "        device(torch.device): the evironment your model training\n",
    "        LR(float): learning rate\n",
    "    '''\n",
    "    # loss function\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    optimizer = optimizer\n",
    "    \n",
    "    # save the mean value of loss in an epoch\n",
    "    running_loss = []\n",
    "    \n",
    "    running_accuracy = []\n",
    "    \n",
    "    # count loss in an epoch \n",
    "    temp_loss = 0.0\n",
    "    \n",
    "    # count the iteration number in an epoch\n",
    "    iteration = 0 \n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        \n",
    "        '''\n",
    "        adjust learning rate when you are training the model\n",
    "        '''\n",
    "        # adjust learning rate\n",
    "        # if epoch % 100 == 0 and epoch != 0:\n",
    "        #     LR = LR * 0.1\n",
    "        #     for param_group in optimizer.param_groups:\n",
    "        #         param_group['lr'] = LR\n",
    "\n",
    "        for i, data in enumerate(trainloader):\n",
    "            point, label = data['point'], data['label']\n",
    "            point, label = point.to(device).to(torch.float32), label.to(device)\n",
    "            outputs = classifier_net(point)\n",
    "            '''# TODO'''\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            '''# TODO END'''\n",
    "            \n",
    "            # save loss in a list\n",
    "            temp_loss += loss.item()\n",
    "            iteration +=1\n",
    "            # print loss value \n",
    "#             print('[{0:d},{1:5.0f}] loss {2:.5f}'.format(epoch + 1, i, loss.item()))\n",
    "            #slow down speed of print function\n",
    "            # time.sleep(0.5)\n",
    "        running_loss.append(temp_loss / iteration)\n",
    "        temp_loss = 0\n",
    "        iteration = 0\n",
    "        print('test {}:----------------------------------------------------------------'.format(epoch))\n",
    "        \n",
    "        # call test function and return accuracy\n",
    "        running_accuracy.append(predict(classifier_net, testloader, device))\n",
    "    \n",
    "    # show loss curve\n",
    "    show_running_loss(running_loss)\n",
    "    \n",
    "    # show accuracy curve\n",
    "    show_accuracy(running_accuracy)\n",
    "    \n",
    "    return classifier_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题2:\n",
    "根据本节前面提到的训练一个网络的完整流程，将下面的代码按照正确的顺序填写到train()函数里面的 **\\# TODO** 里面  \n",
    "\n",
    "\\# update paraeters in optimizer(update weigtht)  \n",
    "**optimizer.step()**  \n",
    "\n",
    "\n",
    "\\# calcutate loss value  \n",
    "**loss = criterion(outputs, label)** \n",
    "\n",
    "\n",
    "\\# empty parameters in optimizer  \n",
    "**optimizer.zero_grad()**  \n",
    "\n",
    "            \n",
    "\\# back propagation  \n",
    "**loss.backward()**  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show running loss curve, you can skip this cell.\n",
    "def show_running_loss(running_loss):\n",
    "    # generate x value\n",
    "    x = np.array([i for i in range(len(running_loss))])\n",
    "    # generate y value\n",
    "    y = np.array(running_loss)\n",
    "    # define a graph\n",
    "    plt.figure()\n",
    "    # generate curve\n",
    "    plt.plot(x, y, c='b')\n",
    "    # show axis\n",
    "    plt.axis()\n",
    "    # define title\n",
    "    plt.title('loss curve:')\n",
    "    #define the name of x axis\n",
    "    plt.xlabel('step')\n",
    "    plt.ylabel('loss value')\n",
    "    # show graph\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5. Test function\n",
    "Test the performance of your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(classifier_net, testloader, device):\n",
    "#     correct = [0 for i in range(3)]\n",
    "#     total = [0 for i in range(3)]\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        '''\n",
    "        you can also stop autograd from tracking history on Tensors with .requires_grad=True \n",
    "        by wrapping the code block in with torch.no_grad():\n",
    "        '''\n",
    "        for data in testloader:\n",
    "            point, label = data['point'], data['label']\n",
    "            point, label = point.to(device).to(torch.float32), label.to(device)\n",
    "            outputs = classifier_net(point)\n",
    "            '''\n",
    "            if you want to get probability of the model prediction,\n",
    "            you can use softmax function here to transform outputs to probability.\n",
    "            '''\n",
    "            # transform the prediction to one-hot form\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            print('model prediction: ', predicted)\n",
    "            print('ground truth:', label, '\\n')\n",
    "            correct += (predicted == label).sum()\n",
    "            total += label.size(0)\n",
    "            print('current correct is:', correct.item())\n",
    "            print('current total is:', total)\n",
    "            \n",
    "        print('the accuracy of the model is {0:5f}'.format(correct.item()/total))\n",
    "        \n",
    "    return correct.item() / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show accuracy curve, you can skip this cell.\n",
    "def show_accuracy(running_accuracy):\n",
    "    x = np.array([i for i in range(len(running_accuracy))])\n",
    "    y = np.array(running_accuracy)\n",
    "    plt.figure()\n",
    "    plt.plot(x, y, c='b')\n",
    "    plt.axis()\n",
    "    plt.title('accuracy curve:')\n",
    "    plt.xlabel('step')\n",
    "    plt.ylabel('accuracy value')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.6. Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    change train epoches here\n",
    "    '''\n",
    "    # number of training\n",
    "    epoches = 100\n",
    "    \n",
    "    '''\n",
    "    change learning rate here\n",
    "    '''\n",
    "    # learning rate\n",
    "    # 1e-4 = e^-4\n",
    "    lr = 1e-3\n",
    "    \n",
    "    '''\n",
    "    change batch size here\n",
    "    '''\n",
    "    # batch size\n",
    "    batch_size = 16\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # define a transform to pretreat data\n",
    "    transform = torch.tensor\n",
    "    \n",
    "    # define a gpu device\n",
    "    device = torch.device('cuda:0')\n",
    "    \n",
    "    # define a trainset\n",
    "    trainset = PointDataset('./labels/train.csv', transform=transform)\n",
    "    \n",
    "    # define a trainloader\n",
    "    trainloader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    \n",
    "    # define a testset\n",
    "    testset = PointDataset('./labels/test.csv', transform=transform)\n",
    "    \n",
    "    # define a testloader\n",
    "    testloader = DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    show_original_points()\n",
    "\n",
    "    # define a network\n",
    "    classifier_net = Network(2, 5, 3).to(device)   \n",
    "    \n",
    "    '''\n",
    "    change optimizer here\n",
    "    '''    \n",
    "    # define a optimizer\n",
    "    optimizer = optim.SGD(classifier_net.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    # get trained model\n",
    "    classifier_net = train(classifier_net, trainloader, testloader, device, lr, optimizer,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题3:\n",
    "**请尝试调整不同大小的学习率，观察loss曲线和accuracy曲线，并阐述学习率对loss值以及accuracy值的影响和原因。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题4:\n",
    "**请尝试调整不同的batch_size, batch_size=1, batch_size=210,batch_size=（1~210）, 并阐述batch_size对loss值以及accuracy值的影响和原因。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题5:\n",
    "**使用SGD优化器，并尝试momentum=0, momentum=0.9两种情况, 阐述momentum对loss值以及accuracy值的影响。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题6:\n",
    "**尝试使用Adam，Rprop优化器，观察两种曲线，阐述SGD,Adam,Rprop三种优化器对loss值以及accuracy值的影响.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题7(自由发挥):\n",
    "**尝试同时调节以上几种参数，找出你认为最合适的参数（模型收敛得最快），并就此谈谈你的感想.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业要求:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. DATA LOADING AND PROCESSING TUTORIAL\n",
    "# （Further content, read it when you are free）\n",
    "\n",
    "A lot of effort in solving any machine learning problem goes in to preparing the data. PyTorch provides many tools to make data loading easy and hopefully, to make your code more readable. In this tutorial, we will see how to load and preprocess/augment data from a non trivial dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. To run this tutorial, please make sure the following packages are installed:\n",
    "    \n",
    "- scikit-image: For image io and transforms  \n",
    "\n",
    " - sudo apt-get install python-numpy  \n",
    " \n",
    " - sudo apt-get install python-scipy  \n",
    " \n",
    " - sudo apt-get install python-matplotlib \n",
    " \n",
    " - sudo pip install  scikit-image  \n",
    "    \n",
    "- pandas: For easier csv parsing\n",
    "\n",
    " - sudo apt-get install python-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Let’s quickly read the CSV and get the annotations in an (N, 2) array where N is the number of landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a csv file by pandas\n",
    "landmarks_frame = pd.read_csv('data/faces/face_landmarks.csv')\n",
    "\n",
    "n = 0\n",
    "# read image name, image name was saved in column 1.\n",
    "img_name = landmarks_frame.iloc[n, 0]\n",
    "# points were saved in columns from 2 to the end\n",
    "landmarks = landmarks_frame.iloc[n, 1:].as_matrix()\n",
    "# reshape the formate of points\n",
    "landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "\n",
    "print('Image name: {}'.format(img_name))\n",
    "print('Landmarks shape: {}'.format(landmarks.shape))\n",
    "print('First 4 Landmarks: {}'.format(landmarks[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_landmarks(image, landmarks):\n",
    "    \"\"\"Show image with landmarks\"\"\"\n",
    "    plt.imshow(image)\n",
    "    plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "plt.figure()\n",
    "show_landmarks(io.imread(os.path.join('data/faces/', img_name)),\n",
    "               landmarks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceLandmarksDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # combine the relative path of images \n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.landmarks_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:].as_matrix()\n",
    "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        # save all data we may need during training a network in a dict\n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note(very important):\n",
    "to define a dataset, first we must to inherit the class **torch.utils.data.Dataset**. when we write ourselves dataset, it's neccesarry for us to overload the **\\_\\___init____** method, **\\_\\___len____** method, and **\\_\\___getitem____** method. Of course you can define other method as you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Let’s instantiate this class and iterate through the data samples. We will print the sizes of first 4 samples and show their landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_dataset = FaceLandmarksDataset(csv_file='data/faces/face_landmarks.csv',\n",
    "                                    root_dir='data/faces/')\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(len(face_dataset)):\n",
    "    sample = face_dataset[i]\n",
    "\n",
    "    print(i, sample['image'].shape, sample['landmarks'].shape)\n",
    "    \n",
    "    # create subgraph\n",
    "    ax = plt.subplot(1, 4, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Sample #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "    show_landmarks(**sample)\n",
    "\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. More materials next week you may need\n",
    "1. [Training a Classifier](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
    "2. [Save Model and Load Model](https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
    "3. [Visualize your training phase](https://github.com/lanpa/tensorboardX)\n",
    "4. [Exploding and Vanishing Gradients](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf)\n",
    "5. [Gradient disappearance and gradient explosion in neural network training](https://bzdww.com/article/19659/)\n",
    "6. [tensorboardX](https://github.com/lanpa/tensorboardX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
